{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mido import MidiFile\n",
    "\n",
    "def load_midi_files(directory):\n",
    "    midi_files = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".mid\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            try:\n",
    "                midi = MidiFile(filepath)\n",
    "                midi_files.append(midi)\n",
    "                print(f\"Loaded: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {filename}: {e}\")\n",
    "    \n",
    "    return midi_files\n",
    "\n",
    "# Specify the directory containing the MIDI files\n",
    "directory = \"mid_files\"\n",
    "\n",
    "# Load all MIDI files\n",
    "midi_files = load_midi_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mido import MidiFile, MetaMessage\n",
    "\n",
    "def gather_midi_insights(midi_files):\n",
    "    insights = []\n",
    "\n",
    "    for midi in midi_files:\n",
    "        midi_info = {\n",
    "            'filename': midi.filename,\n",
    "            'num_tracks': len(midi.tracks),\n",
    "            'total_length_sec': midi.length,\n",
    "            'tempo': None,\n",
    "            'time_signature': None,\n",
    "            'key_signature': None,\n",
    "            'instruments': set(),\n",
    "            'num_notes': 0,\n",
    "            'unique_pitches': set(),\n",
    "        }\n",
    "        \n",
    "        for i, track in enumerate(midi.tracks):\n",
    "            for msg in track:\n",
    "                if msg.type == 'set_tempo':\n",
    "                    midi_info['tempo'] = msg.tempo\n",
    "                elif msg.type == 'time_signature':\n",
    "                    midi_info['time_signature'] = f\"{msg.numerator}/{msg.denominator}\"\n",
    "                elif msg.type == 'key_signature':\n",
    "                    midi_info['key_signature'] = msg.key\n",
    "                elif msg.type == 'program_change':\n",
    "                    midi_info['instruments'].add(msg.program)\n",
    "                elif msg.type == 'note_on' and msg.velocity > 0:\n",
    "                    midi_info['num_notes'] += 1\n",
    "                    midi_info['unique_pitches'].add(msg.note)\n",
    "        \n",
    "        insights.append(midi_info)\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Gather insights from the loaded MIDI files\n",
    "midi_insights = gather_midi_insights(midi_files)\n",
    "\n",
    "# Display gathered insights\n",
    "for insight in midi_insights:\n",
    "    print(f\"\\nFile: {insight['filename']}\")\n",
    "    print(f\" - Number of Tracks: {insight['num_tracks']}\")\n",
    "    print(f\" - Total Length (seconds): {insight['total_length_sec']:.2f}\")\n",
    "    print(f\" - Tempo: {insight['tempo']}\")\n",
    "    print(f\" - Time Signature: {insight['time_signature']}\")\n",
    "    print(f\" - Key Signature: {insight['key_signature']}\")\n",
    "    print(f\" - Instruments (Program Numbers): {insight['instruments']}\")\n",
    "    print(f\" - Number of Notes: {insight['num_notes']}\")\n",
    "    print(f\" - Unique Pitches: {sorted(insight['unique_pitches'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_midi_insights(midi_insights):\n",
    "    # Convert the insights into a pandas DataFrame\n",
    "    df = pd.DataFrame(midi_insights)\n",
    "    \n",
    "    # Number of Tracks per MIDI File\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='filename', y='num_tracks', data=df, palette='Blues_d')\n",
    "    plt.title('Number of Tracks per MIDI File')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Number of Tracks')\n",
    "    plt.xlabel('MIDI Files')\n",
    "    plt.show()\n",
    "    \n",
    "    # Total Length of MIDI Files\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='filename', y='total_length_sec', data=df, palette='Greens_d')\n",
    "    plt.title('Total Length of MIDI Files (seconds)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Total Length (seconds)')\n",
    "    plt.xlabel('MIDI Files')\n",
    "    plt.show()\n",
    "    \n",
    "    # Distribution of Tempos\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['tempo'].dropna(), bins=10, kde=True, color='purple')\n",
    "    plt.title('Distribution of Tempos')\n",
    "    plt.xlabel('Tempo (microseconds per beat)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # Number of Notes per MIDI File\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='filename', y='num_notes', data=df, palette='Reds_d')\n",
    "    plt.title('Number of Notes per MIDI File')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Number of Notes')\n",
    "    plt.xlabel('MIDI Files')\n",
    "    plt.show()\n",
    "    \n",
    "    # Unique Pitches Used\n",
    "    df['num_unique_pitches'] = df['unique_pitches'].apply(len)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(y='num_unique_pitches', data=df, color='orange')\n",
    "    plt.title('Distribution of Unique Pitches Used in MIDI Files')\n",
    "    plt.ylabel('Number of Unique Pitches')\n",
    "    plt.xlabel('MIDI Files')\n",
    "    plt.xticks([])\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the gathered insights\n",
    "visualize_midi_insights(midi_insights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "import os\n",
    "\n",
    "def parse_midi_file(filepath):\n",
    "    events = []\n",
    "    try:\n",
    "        mid = mido.MidiFile(filepath)\n",
    "        for i, track in enumerate(mid.tracks):\n",
    "            for msg in track:\n",
    "                if msg.type in ['note_on', 'note_off']:\n",
    "                    # Encode as (event type, note, velocity)\n",
    "                    event_type = 1 if msg.type == 'note_on' else 0\n",
    "                    events.append((event_type, msg.note, msg.velocity))\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing file {filepath}: {e}\")\n",
    "    return events\n",
    "\n",
    "def encode_events(events, event_to_int):\n",
    "    encoded = []\n",
    "    for event in events:\n",
    "        encoded.append(event_to_int[event])\n",
    "    return encoded\n",
    "\n",
    "def preprocess_midi_files(directory):\n",
    "    all_sequences = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".mid\") or filename.endswith(\".midi\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            events = parse_midi_file(filepath)\n",
    "            if events:  # Only add non-empty event lists\n",
    "                all_sequences.append(events)\n",
    "    \n",
    "    return all_sequences\n",
    "\n",
    "# Parse and encode MIDI files\n",
    "directory = \"mid_files\"\n",
    "sequences = preprocess_midi_files(directory)\n",
    "\n",
    "def get_dimensions(lst):\n",
    "    if isinstance(lst, list) and lst:  # Check if it's a list and not empty\n",
    "        return [len(lst)] + get_dimensions(lst[0])\n",
    "    return []\n",
    "\n",
    "# Calculate the total number of tracks\n",
    "total_tracks = sum(len(seq) for seq in sequences)\n",
    "\n",
    "print(get_dimensions(sequences))\n",
    "print(f\"Total number of tracks = {total_tracks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a set to collect unique tuples\n",
    "unique_tuples = set()\n",
    "\n",
    "# Iterate through the 2D array and add each tuple to the set\n",
    "for row in sequences:\n",
    "    for tuple_element in row:\n",
    "        unique_tuples.add(tuple_element)\n",
    "\n",
    "# Convert the set to a sorted list if needed\n",
    "unique_tuples = sorted(unique_tuples)\n",
    "\n",
    "# Print or return the results\n",
    "print(\"Unique sounds:\", len(unique_tuples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_to_int_mapping(sequences):\n",
    "    event_set = set()\n",
    "    for seq in sequences:\n",
    "        event_set.update(seq)\n",
    "    \n",
    "    event_to_int = {event: i for i, event in enumerate(sorted(event_set))}\n",
    "    int_to_event = {i: event for event, i in event_to_int.items()}\n",
    "    return event_to_int, int_to_event\n",
    "\n",
    "event_to_int, int_to_event = create_event_to_int_mapping(sequences)\n",
    "\n",
    "# Encode all sequences\n",
    "encoded_sequences = [encode_events(seq, event_to_int) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_len, padding_value=0):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [padding_value] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        padded_sequences.append(seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "# Set maximum sequence length\n",
    "max_seq_length = 1000  # You can adjust this based on your data\n",
    "\n",
    "# Pad/Truncate sequences\n",
    "padded_sequences = pad_sequences(encoded_sequences, max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDIDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 8\n",
    "dataset = MIDIDataset(padded_sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, forward_expansion, dropout, max_length):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_length, embed_size))\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=forward_expansion * embed_size,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, tgt):\n",
    "        # Embed and add positional encoding\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        tgt = self.dropout(tgt)\n",
    "\n",
    "        # Forward pass through the transformer\n",
    "        transformer_out = self.transformer(x, tgt)\n",
    "        out = self.fc_out(transformer_out)\n",
    "        return out\n",
    "\n",
    "print(len(event_to_int))\n",
    "# Model Hyperparameters\n",
    "vocab_size = len(event_to_int)  # Number of unique MIDI events\n",
    "embed_size = 128\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "forward_expansion = 4\n",
    "dropout = 0.2\n",
    "max_length = max_seq_length  # This should match the sequence length used in preprocessing\n",
    "\n",
    "# Initialize the model\n",
    "model = MusicTransformer(\n",
    "    vocab_size, embed_size, num_heads, num_layers, forward_expansion, dropout, max_length\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Input and target sequences\n",
    "        x = batch[:, :-1]  # Input sequence\n",
    "        y = batch[:, 1:]   # Target sequence (shifted by one time step)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(x, x)\n",
    "        \n",
    "        # Reshape output and target to be compatible with CrossEntropyLoss\n",
    "        output = output.reshape(-1, vocab_size)  # Use .reshape instead of .view\n",
    "        y = y.reshape(-1)  # Use .reshape instead of .view\n",
    "        \n",
    "        # Calculate loss and backpropagate\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = 'music_generator.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# To load the model later\n",
    "model = MusicTransformer()  # Replace with your model class\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model.eval()\n",
    "print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_music(model, seed_sequence, max_length, int_to_event, temperature=1.0, top_k=10, device='cuda'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated_sequence = seed_sequence.copy()  # Start with the seed sequence\n",
    "\n",
    "    # Convert seed sequence to tensor and ensure it has the correct dimensions\n",
    "    x = torch.tensor(seed_sequence, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed_sequence)):\n",
    "            # Predict the next event\n",
    "            output = model(x, x)  # Autoregressive prediction\n",
    "            output = output[:, -1, :]  # Get the output for the last time step\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            output = output / temperature\n",
    "\n",
    "            # Apply top-k filtering\n",
    "            top_k_values, top_k_indices = torch.topk(output, top_k, dim=-1)\n",
    "            top_k_probs = torch.nn.functional.softmax(top_k_values, dim=-1)\n",
    "\n",
    "            # Sample from the top-k probable events\n",
    "            next_event_idx = top_k_indices[0, torch.multinomial(top_k_probs, 1).item()].item()\n",
    "\n",
    "            # Append the predicted event to the sequence\n",
    "            generated_sequence.append(next_event_idx)\n",
    "\n",
    "            # Update the input tensor with the new event\n",
    "            x = torch.tensor(generated_sequence[-x.size(1):], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Convert generated sequence back to events\n",
    "    generated_events = [int_to_event[idx] for idx in generated_sequence]\n",
    "\n",
    "    return generated_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Step 1: Generate a random length between 1 and 500\n",
    "seed_length = random.randint(1, 500)\n",
    "\n",
    "# Step 2: Randomly select tuples to create the seed sequence\n",
    "seed_sequence = [random.choice(unique_tuples) for _ in range(seed_length)]\n",
    "\n",
    "# Print or use the seed sequence\n",
    "print(\"Generated seed sequence:\", seed_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_seed = [encode_events(seed_sequence, event_to_int)]\n",
    "padded_seed = pad_sequences(encoded_seed, max_seq_length)[0]\n",
    "\n",
    "max_length = 100  # Set the maximum length of the generated sequence\n",
    "\n",
    "# Generate music\n",
    "generated_sequence = generate_music(\n",
    "    model,\n",
    "    padded_seed,\n",
    "    max_length,\n",
    "    int_to_event,\n",
    "    temperature=1.5,  # Adjust temperature for more/less randomness\n",
    "    top_k = 10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Print the generated events\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize Pygame Mixer\n",
    "pygame.mixer.init(frequency=22050, size=-16, channels=1)\n",
    "\n",
    "# Function to convert MIDI note number to frequency\n",
    "def midi_to_frequency(midi_note):\n",
    "    A440 = 440.0\n",
    "    return A440 * 2 ** ((midi_note - 69) / 12.0)\n",
    "\n",
    "# Function to generate a sound wave with multiple sweet-sounding waveforms and ADSR envelope\n",
    "def generate_tone(frequency, duration, sample_rate=22050, fade_out_duration=0.05, seed=42):\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    \n",
    "    # Initialize random number generator with a seed\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Define sweet-sounding waveforms\n",
    "    sine_wave = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
    "    triangle_wave = 0.5 * (2 / np.pi) * np.arcsin(np.sin(2 * np.pi * frequency * t))\n",
    "    sawtooth_wave = 0.5 * (2 * (t * frequency - np.floor(0.5 + t * frequency)))\n",
    "    square_wave = 0.5 * np.sign(np.sin(2 * np.pi * frequency * t))\n",
    "    harmonic_sine_wave = 0.5 * np.sin(2 * np.pi * frequency * t * 2)  # Harmonic sine wave\n",
    "    low_freq_sine_wave = 0.5 * np.sin(2 * np.pi * frequency * t * 0.5)  # Low-frequency sine wave\n",
    "    \n",
    "    # Gaussian wave (bell-like sound)\n",
    "    gaussian_wave = 0.5 * np.exp(-0.5 * ((t - duration / 2) / (duration / 6))**2)\n",
    "    \n",
    "    # Complex sine wave (sum of multiple sine waves)\n",
    "    complex_sine_wave = 0.25 * (np.sin(2 * np.pi * frequency * t) +\n",
    "                                np.sin(2 * np.pi * frequency * 2 * t) +\n",
    "                                np.sin(2 * np.pi * frequency * 3 * t))\n",
    "    \n",
    "    # List of sweet-sounding waveforms\n",
    "    sweet_waves = [\n",
    "        sine_wave, triangle_wave, sawtooth_wave, square_wave, \n",
    "        harmonic_sine_wave, low_freq_sine_wave, \n",
    "        gaussian_wave, complex_sine_wave\n",
    "    ]\n",
    "    \n",
    "    # Randomly select a combination of waveforms\n",
    "    num_waveforms = random.randint(1, len(sweet_waves))  # Choose 1 to all waveforms\n",
    "    selected_waveforms = random.sample(sweet_waves, num_waveforms)\n",
    "    \n",
    "    # Mix waveforms\n",
    "    waveform = np.mean(selected_waveforms, axis=0)\n",
    "    \n",
    "    # Apply ADSR envelope\n",
    "    attack = 0.05  # Attack time in seconds\n",
    "    decay = 0.1    # Decay time in seconds\n",
    "    sustain_level = 0.7  # Sustain level (0 to 1)\n",
    "    sustain_duration = duration - attack - decay - fade_out_duration\n",
    "    release = fade_out_duration\n",
    "    \n",
    "    adsr = np.concatenate([\n",
    "        np.linspace(0, 1, int(sample_rate * attack)),  # Attack\n",
    "        np.linspace(1, sustain_level, int(sample_rate * decay)),  # Decay\n",
    "        np.full(int(sample_rate * sustain_duration), sustain_level),  # Sustain\n",
    "        np.linspace(sustain_level, 0, int(sample_rate * release))  # Release\n",
    "    ])\n",
    "    \n",
    "    # Ensure the ADSR envelope matches the waveform length\n",
    "    if len(adsr) < len(waveform):\n",
    "        adsr = np.concatenate([adsr, np.zeros(len(waveform) - len(adsr))])\n",
    "    elif len(adsr) > len(waveform):\n",
    "        adsr = adsr[:len(waveform)]\n",
    "    \n",
    "    waveform *= adsr\n",
    "    \n",
    "    waveform = np.int16(waveform * 32767)  # Convert to 16-bit PCM format\n",
    "    return waveform\n",
    "\n",
    "# Function to play a tone\n",
    "def play_tone(frequency, duration):\n",
    "    tone_data = generate_tone(frequency, duration)\n",
    "    sound = pygame.sndarray.make_sound(tone_data)\n",
    "    sound.play()\n",
    "    pygame.time.wait(int(duration * 1000))  # Wait for the sound to finish\n",
    "\n",
    "# Function to play a sequence of MIDI events with varied tones\n",
    "def play_sequence(events, duration_per_note=0.5):\n",
    "    for event in events:\n",
    "        _, midi_note, velocity = event\n",
    "        frequency = midi_to_frequency(midi_note)\n",
    "        \n",
    "        # Optionally, you could vary other parameters like duration here\n",
    "        play_tone(frequency, duration_per_note)\n",
    "\n",
    "\n",
    "play_sequence(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
