{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b62b9a-eb85-4dca-9770-f5cf2c10da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from music21 import converter, instrument, note, chord\n",
    "\n",
    "def load_abc_files(directory):\n",
    "    melody_data = []\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory)[:100]:\n",
    "        try:\n",
    "            if filename.endswith('.abc'):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                # Parse the ABC file\n",
    "                score = converter.parse(file_path)\n",
    "    \n",
    "                # Extract notes and durations\n",
    "                for element in score.flatten().notes:\n",
    "                    if isinstance(element, note.Note):\n",
    "                        melody_data.append((str(element.nameWithOctave), str(element.quarterLength)))\n",
    "                    elif isinstance(element, chord.Chord):\n",
    "                        # For chords, take the first note and its duration\n",
    "                        melody_data.append((str(element.notes[0].nameWithOctave), str(element.quarterLength)))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return melody_data\n",
    "\n",
    "# Load ABC files\n",
    "abc_directory = 'abc_files'\n",
    "melody_tokens = load_abc_files(abc_directory)\n",
    "\n",
    "# Format the data into the desired 1D array\n",
    "formatted_melody = []\n",
    "for note, duration in melody_tokens:\n",
    "    formatted_melody.append(note)\n",
    "    formatted_melody.append(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e603c0a-8206-4282-b533-423b20eb6dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31 31  6 ...  8 12 11]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Separate notes and durations\n",
    "notes = formatted_melody[::2]  # Every second item starting from index 0\n",
    "durations = formatted_melody[1::2]  # Every second item starting from index 1\n",
    "\n",
    "# Encode notes and durations\n",
    "note_encoder = LabelEncoder()\n",
    "duration_encoder = LabelEncoder()\n",
    "\n",
    "note_encoder.fit(notes)\n",
    "duration_encoder.fit(durations)\n",
    "\n",
    "# Convert to numerical format\n",
    "encoded_notes = note_encoder.transform(notes)\n",
    "encoded_durations = duration_encoder.transform(durations)\n",
    "\n",
    "# Combine into a single array\n",
    "tokens = np.array([encoded_notes, encoded_durations]).flatten()\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb9f26c-eb21-4fed-a0d1-d18dc6c8b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 31  6 ... 19  6 32]\n",
      " [31  6 19 ...  6 32 40]\n",
      " [ 6 19  6 ... 32 40 28]\n",
      " ...\n",
      " [ 8 12  2 ...  2  0  8]\n",
      " [12  2  0 ...  0  8 12]\n",
      " [ 2  0  0 ...  8 12 11]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10  # Length of each input sequence\n",
    "sequences = []\n",
    "for i in range(len(tokens) - seq_length):\n",
    "    sequences.append(tokens[i:i + seq_length + 1])  # +1 for the target\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca36648-8e98-49ef-a810-78d27ec7f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MelodyTransformer(\n",
       "  (embedding): Embedding(156, 32)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=32, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=32, bias=True)\n",
       "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "          (dropout3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=32, out_features=156, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MelodyTransformer(nn.Module):\n",
    "    def __init__(self, n_tokens, d_model=32, n_heads=1, n_layers=1, dropout=0.5):\n",
    "        super(MelodyTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_tokens, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 500, d_model))  # Adjust max length as needed\n",
    "        self.transformer = nn.Transformer(d_model, n_heads, num_encoder_layers=n_layers, num_decoder_layers=n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(d_model, n_tokens)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        \n",
    "        output = self.transformer(src_emb, tgt_emb)\n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "n_tokens = len(note_encoder.classes_) + len(duration_encoder.classes_)\n",
    "model = MelodyTransformer(n_tokens)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17f8271-fc31-4f5d-8363-b8db23c052e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1098\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Prepare dataset\n",
    "X = torch.tensor(sequences[:, :-1], dtype=torch.long).to(device)  # Input sequences on GPU\n",
    "y = torch.tensor(sequences[:, 1:], dtype=torch.long).to(device)   # Target sequences on GPU\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)  # Move batch data to GPU\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(batch_X, batch_y)  # Pass src and tgt to the model\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output.view(-1, n_tokens), batch_y.view(-1))  # Reshape for loss calculation\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbdcd548-e48a-49e6-8476-184a1cb4e1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Melody: ['E4', '13.0', 'A4', '10.5', 'A4', '13.125', 'E4', '10.5', 'C5', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'A4', '0.875', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25', 'B-5', '1.25']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def convert_to_note_duration(generated_sequence, note_encoder, duration_encoder):\n",
    "    generated_notes = note_encoder.inverse_transform(generated_sequence[::2])\n",
    "    generated_durations = duration_encoder.inverse_transform(generated_sequence[1::2])\n",
    "\n",
    "    melody = []\n",
    "    for note, duration in zip(generated_notes, generated_durations):\n",
    "        melody.append(str(note))\n",
    "        melody.append(str(duration))\n",
    "\n",
    "    return melody\n",
    "\n",
    "def apply_duration_penalty(logits, max_duration=16.0):\n",
    "    # Create a penalty factor based on the duration values\n",
    "    # Assuming the logits correspond to the indices of durations\n",
    "    duration_penalty = torch.arange(len(logits)).float()  # Create a tensor of indices\n",
    "    penalty = (duration_penalty / max_duration) ** 2  # Example penalty: quadratic scaling\n",
    "    penalty = penalty.to(logits.device)  # Move penalty to the same device as logits\n",
    "\n",
    "    # Apply the penalty to the logits\n",
    "    logits -= penalty  # Subtract penalty from logits to reduce the probability of higher durations\n",
    "    return logits\n",
    "\n",
    "def generate_melody(model, seed, length=20, device='cpu', temperature=1.0, top_k=None, valid_tokens=None):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    generated = seed.tolist()  # Start with the seed sequence\n",
    "    input_seq = torch.tensor(seed, dtype=torch.long).unsqueeze(0).to(device)  # Shape: (1, seq_length)\n",
    "\n",
    "    # Set to track recent sequences\n",
    "    recent_sequences = set()\n",
    "    max_repeats = 3\n",
    "\n",
    "    n_sequences = 0\n",
    "    while n_sequences < length:\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            output = model(input_seq, input_seq)\n",
    "            logits = output[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply duration penalty\n",
    "            logits = apply_duration_penalty(logits)\n",
    "\n",
    "            # Apply top-k sampling if specified\n",
    "            if top_k is not None:\n",
    "                top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "                top_k_probs = F.softmax(top_k_values, dim=-1)\n",
    "                next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "                next_token = top_k_indices[0, next_token]  # Get the original token index\n",
    "            else:\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "            # Convert next_token to a simple integer\n",
    "            next_token = next_token.item()\n",
    "\n",
    "            # Convert next_token to its corresponding string label\n",
    "            if next_token < len(note_encoder.classes_):\n",
    "                if next_token >= len(note_encoder.classes_):\n",
    "                    continue\n",
    "                next_token_str = note_encoder.classes_[next_token]\n",
    "            else:\n",
    "                if next_token >= next_token - len(note_encoder.classes_):\n",
    "                    continue\n",
    "                next_token_str = duration_encoder.classes_[next_token - len(note_encoder.classes_)]\n",
    "\n",
    "            # Check if the generated token is in the list of valid tokens\n",
    "            if next_token_str not in valid_tokens:\n",
    "                print(\"invalid_token\")\n",
    "                continue  # Skip this token if it's not valid\n",
    "\n",
    "            # Check for recent sequences\n",
    "            generated_sequence = tuple(generated[-2:] + [next_token])  # Consider last 2 + next token\n",
    "            if generated_sequence in recent_sequences:\n",
    "                print(\"repeats\")\n",
    "                # If the sequence is already in the recent set, skip this token\n",
    "                continue\n",
    "\n",
    "            # Add the generated token to the list\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # Update recent sequences\n",
    "            recent_sequences.add(generated_sequence)\n",
    "            if len(recent_sequences) > max_repeats:\n",
    "                recent_sequences.pop()  # Keep the set size manageable\n",
    "\n",
    "            # Update input_seq for the next iteration\n",
    "            input_seq = torch.cat((input_seq, torch.tensor([[next_token]], dtype=torch.long).to(device)), dim=1)\n",
    "            n_sequences += 1\n",
    "            # print(f\"Progress: {(n_sequences/length)*100}%\")\n",
    "    return generated\n",
    "\n",
    "seed = sequences[0, :-1]  # Use the first sequence as a seed\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Ensure the device is set\n",
    "\n",
    "valid_tokens = np.concatenate((note_encoder.classes_, duration_encoder.classes_)).tolist()\n",
    "generated_sequence = generate_melody(model, seed, length=200, device=device, temperature=1.5, top_k=10, valid_tokens=valid_tokens)\n",
    "\n",
    "# Decoding the generated sequence\n",
    "melody = convert_to_note_duration(generated_sequence, note_encoder, duration_encoder)\n",
    "print(\"Generated Melody:\", melody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7a783-b777-4e6b-a4ee-90a19c9c229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream, note, meter, tempo, midi\n",
    "\n",
    "def melody_to_midi(melody, output_file='generated_melody.mid', tempo_bpm=120, time_signature='4/4'):\n",
    "    # Create a music21 stream\n",
    "    melody_stream = stream.Stream()\n",
    "\n",
    "    # Add a tempo mark\n",
    "    melody_stream.append(tempo.MetronomeMark(number=tempo_bpm))\n",
    "\n",
    "    # Add a time signature\n",
    "    time_signature_parts = time_signature.split('/')\n",
    "    time_signature_obj = meter.TimeSignature(time_signature)\n",
    "    melody_stream.append(time_signature_obj)\n",
    "\n",
    "    # Iterate over the melody and create notes and durations\n",
    "    for i in range(0, len(melody), 2):\n",
    "        note_name = melody[i]  # Note\n",
    "        duration_value = float(melody[i + 1])  # Duration\n",
    "\n",
    "        # Create a music21 note\n",
    "        new_note = note.Note(note_name)\n",
    "        new_note.quarterLength = duration_value  # Set the duration\n",
    "\n",
    "        # Append the note to the stream\n",
    "        melody_stream.append(new_note)\n",
    "\n",
    "    # Write the stream to a MIDI file\n",
    "    melody_stream.write('midi', fp=output_file)\n",
    "    print(f'MIDI file saved as {output_file}')\n",
    "\n",
    "melody_to_midi(melody, output_file='generated_melody.mid', tempo_bpm=360, time_signature='4/4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca771f08-40f3-495b-bc81-7f10e92fdf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
